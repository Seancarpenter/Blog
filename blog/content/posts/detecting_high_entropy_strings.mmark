+++
title = "Detecting High Entropy Sequences"
author = "Sean Carpenter"
date = "2020-02-02"
description = "A statisitcal approach to finding randomly generated strings."
featured_image = "posts/2020/set_cover_and_aliens/alien_abduction.png"
libraries = ["katex"]
+++

### Introduction

The entropy of a sequence is defined as the amount of bits needed to store the amount of information of said sequence. For example, if we wish to store the contents of a coin flip, be it heads or tails, we'd only need need 1 bit; a value of 1 could signal that the coin landed on heads, while a value of 0 could signal that the coin landed on tails. However, this same idea can be extended to represent the irregularity of a given sequence in relation to another (typically much larger) set of sequences. Take the following example:

```
aardvark, abadon, abdicate, pvreskzgoqrxyjfsqylc, abide, ability, abject
```

At a glance, it should be clear that `pvreskzgoqrxyjfsqylc` is the odd one out, and if we look at the source of each of these sequences, it should be easy to see why. `aardvark, abadon, abdicate, abide, ability` and `abject` were all pulled from the first page of the nearest dictionary I could find, and `pvreskzgoqrxyjfsqylc` was randomly generated using a simple Python function I wrote.

{{< highlight python >}}
import string
import random

def generate_random_string(length):
    return "".join(string.ascii_lowercase) for _ in range(length))
{{< /highlight >}}

```
>>> generate_random_string(20)
pvreskzgoqrxyjfsqylc
```

Mathematically speaking however, what actually makes `pvreskzgoqrxyjfsqylc` stand out from the rest of the strings in the set? Humans are exceptional at recognizing strings and sequences (this one holds meaning, this one does not), but a computer has no latent ability to understand that an `aardvark` is a small nocturnal mammal that feeds mostly on ants and termites, and that `pvreskzgoqrxyjfsqylc` is a meaningless jumble of characters.

However, there are two readily available heurisitics we can use to compare`aardvark` and `pvreskzgoqrxyjfsqylc` without any outside knowledge of the physical world or the English language. The first is the length of these two strings.

```
>>> strings = ["aardvark", "abadon", "abdicate", "pvreskzgoqrxyjfsqylc", "abide", "ability", "abject"]
>>> average_length = sum([len(s) for s in strings]) / len(strings)
>>> average_length
8.571428571428571
```

At 20 characters long, `pvreskzgoqrxyjfsqylc` is more than twice as long as the average string in our set, while `aardvark`, at 8 characters long, is about as close to the average as any string can get. Given that most of the words in the english language are far shorter than 20 characters in length, this dramatic difference in length is ultimately something we'll want to take into account.

The second heuristic we can use is the probability of each individual character occuring within a given string in relation to the larger set of strings. In order to determine this, we'll need to build a simple probability distribution table. This is easily accomplished by iterating through our set of strings, and counting how many times each character occurs.

{{< highlight python >}}
VALID_CHARS = set(string.ascii_lowercase)

def generate_pd_table(strings):
    pd_table = {c: 0 for c in VALID_CHARS}
    num_characters = 0
    for s in strings:
        for c in s:
            if c in VALID_CHARS:
                num_characters += 1
                pd_table[c] += 1

    return {c: pd_table[c] / max(1, num_characters) for c in VALID_CHARS}

def print_pd_table(pd_table):
    for c in sorted(pd_table.keys(), key=lambda c: pd_table[c], reverse=True):
        bar = ("█" * (int(pd_table[c] * 100 / 0.5))).strip()
        print(f"{c}: {bar} {pd_table[c] * 100:.2f}%")

{{< /highlight >}}

```
>>> strings = ["aardvark", "abadon", "abdicate", "pvreskzgoqrxyjfsqylc", "abide", "ability", "abject"]
>>> pd_table = generate_pd_table(strings)
>>> print_pd_table(pd_table)

a: █████████████████████████████████ 16.67%
b: ████████████████ 8.33%
d: █████████████ 6.67%
e: █████████████ 6.67%
i: █████████████ 6.67%
r: █████████████ 6.67%
c: ██████████ 5.00%
t: ██████████ 5.00%
y: ██████████ 5.00%
j: ██████ 3.33%
k: ██████ 3.33%
l: ██████ 3.33%
o: ██████ 3.33%
q: ██████ 3.33%
s: ██████ 3.33%
v: ██████ 3.33%
f: ███ 1.67%
g: ███ 1.67%
n: ███ 1.67%
p: ███ 1.67%
x: ███ 1.67%
z: ███ 1.67%
h: 0.00%
m: 0.00%
u: 0.00%
w: 0.00%
```

At a glance, it's easy to tell that some letters occur more frequently than others. Given that the majority of strings in our set were all chosen from the English dictionary in alphabetical order, a bias towards vowels like `a` and `e`, and consonants like `b` and `d` , over consonants like `z` and `w` is to be expected. Additionally, as can be exhibited by the following probability distribution table generated from a [relatively comprehensive list of words in the English language](https://github.com/Seancarpenter/blog-content/blob/master/2020/detecting_secrets_using_entropy/english.txt), these dramatic differences in probability are not isolated to our comparitvely small set of strings.

{{< highlight python >}}
def ingest_text(file_name):
    with open(file_name) as file:
        strings = file.read().replace("\n", " ").split(" ")
    return strings
{{< /highlight >}}

```
>>> strings = ingest_text("english.txt")
>>> pd_table = generate_pd_table(strings)
>>> print_pd_table(pd_table)

e: ███████████████████████ 11.61%
s: █████████████████ 8.91%
i: █████████████████ 8.76%
a: ███████████████ 7.61%
r: ██████████████ 7.38%
n: █████████████ 6.94%
t: █████████████ 6.65%
o: ████████████ 6.14%
l: ██████████ 5.38%
c: ████████ 4.05%
d: ███████ 3.67%
u: ██████ 3.34%
g: █████ 2.88%
p: █████ 2.82%
m: █████ 2.75%
h: ████ 2.22%
b: ███ 1.96%
y: ███ 1.69%
f: ██ 1.37%
v: ██ 1.04%
k: █ 0.87%
w: █ 0.86%
z: 0.44%
x: 0.29%
j: 0.19%
q: 0.18%
```

So, with our heuristics defined, lets take a look at actually using them to calculate entropy for an arbitrary string in a larger set of strings.

### Calculating Entropy

At the core of our algorithm is a relatively simple formula known as Shannon Entropy.

$$H(x) = -\sum\limits^n_{i=1} P(x_i)\ log_2\ P(x_i)$$

In this formula,

$$H(x)$$ is the calculated entropy of a given sequence x.

$$P(x_i)$$ is the probability of the $$ith$$ value in the sequence $$x$$ occuring.

By explicitely taking into account the probability of each individual character occuring, and summing each character across the entire length of the string, Shannon Entropy properly factors in both of our heurisitics.

For the sake of brevity, I've left out most of the theory required to understand how exactly Shannon Entropy works, but if you'd like to learn more about the formula, this [brief paper](https://arxiv.org/ftp/arxiv/papers/1405/1405.2061.pdf) by Sriram Vajapeyam is a great place to start. If that isn't enough for you, [*Entropy and Information Theory*](https://ee.stanford.edu/~gray/it.html) by Robert Gray should almost certainly do the trick.

With formula in hand, let's see if we can properly implement it:

{{< highlight python >}}
def entropy(s, pd_table):
    entropy = 0
    for c in s:
        if c in VALID_CHARS and pd_table[c] > 0:
            entropy += (1 / pd_table[c]) * math.log2(1 / pd_table[c])

    return entropy
{{< /highlight>}}

```
>>> entropy("xywjz", pd_table)
10649.261670477576
>>> entropy("apple", pd_table)
518.3318569640584
```
A discerning eye can pinpoint



